{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pyspark.sql.functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "#PARAMETERS\n",
        "raw_folderpath = \"\"\n",
        "raw_filename = \"\"\n",
        "mappingJSON = \"\"\n",
        "sinkdbTableName = \"\"\n",
        "delimiter = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Storage Config\n",
        "storageLinkedService = 'LS_DataLake'\n",
        "storageAccount_ls = mssparkutils.credentials.getPropertiesAll(storageLinkedService)\n",
        "storage_account = json.loads(storageAccount_ls)['Endpoint'].split('.')[0].replace('https://','')\n",
        "container = 'raw'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read Data\n",
        "file_type = raw_filename.split('.')[1]\n",
        "\n",
        "if file_type == 'json':\n",
        "    initialDataframe = spark.read.json('abfss://'+container+'@'+ storage_account + '.dfs.core.windows.net/'+ raw_folderpath+'/'+ raw_filename)\n",
        "elif file_type == 'csv':\n",
        "    initialDataframe = spark.read.csv('abfss://'+container+'@'+ storage_account + '.dfs.core.windows.net/'+ raw_folderpath+'/'+ raw_filename,sep=delimiter,header=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define Mapping Function\n",
        "def MapData(mappingJSON, container, storage_account, folderpath, filename, table):\n",
        "\n",
        "    #Step 1: Parse Mapping JSON\n",
        "    mapping_json = json.loads(mappingJSON)\n",
        "    columnMappingsDict = list(mapping_json['mappings'])\n",
        "    columnMappings = [(columnMapping['source'], columnMapping['sink']) for columnMapping in columnMappingsDict]\n",
        "\n",
        "    #Step 2: Rename Columns\n",
        "    updated_columns = [f.col(mapping[0]).alias(mapping[1]) for mapping in columnMappings]\n",
        "    newDataframe=initialDataframe.select([c.cast('string') for c in updated_columns])\n",
        "\n",
        "    return newDataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Map DF\n",
        "mapped_df = MapData(mappingJSON, container, storage_account, raw_folderpath, raw_filename, sinkdbTableName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Output to Staging\n",
        "mapped_df.write.mode(\"append\").parquet('abfss://'+'staging'+'@'+ storage_account + '.dfs.core.windows.net/EnterpriseTaxModel/'+sinkdbTableName)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
