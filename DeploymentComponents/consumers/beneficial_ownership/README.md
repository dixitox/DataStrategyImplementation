# Steps to Deploy the Beneficial Ownership Consumer
1. Update the [dev variable file](variables/dev.json) for variables related to the new consumer Synapse like resource names and tags 

2. Trigger the **consumer-orchestrator** GitHub Action. If you're unfamiliar with triggering a GitHub Action, follow these [instructions](https://docs.github.com/en/actions/managing-workflow-runs/manually-running-a-workflow).
    - Input **beneficial_ownership** for the "Consumer Name" input

# Post Deployment Tasks - Azure SQL
1. Execute the below stored procedure in the deployed Azure SQL Database from the **consumer-orchestrator** GitHub Action
    - Login with AAD. SQL Auth is disabled.
```sql
EXEC [dbo].[AddManagedIdentitiesAsUsers]
```
2. Execute the below sql script in the Azure SQL Database deployed from the **data-strategy-orchestrator** GitHub Action
    - Login with AAD. SQL Auth is disabled.
```sql
EXEC [dbo].[Execute_For_Beneficial_Owner_Consumer]
```

# Steps to Run the Transparency Engine

1. Make sure all the tables you need are in the curated zone , under a single subfolder, in .csv format. These tables are generated by running the ETL process starting with the ingestion of the open source datasets with ADF that will copy them into landing and the raw zone, and consequent synapse pipelines that will move them from raw to curated applying the necessary validation and formatting operations to bring them in the format expected by the transparency engine. The tables needed from the transparency engine are : activity, contact, ownership, attributedefinition, redflagdefinition , entityweight. 
   
2. Make sure the .whl and .env file are installed on the Spark cluster that will be used to run the transparency engine notebook
 
3. In the notebook, update the variables of folderpath, storagename, and datecountry. Folderpath need to match the subfolder name in curated where the .csv are placed, storagename is the storage account name, and datecountry will be the name of the lakedb where the final reports will be generated, that will be determined by the user. If datecountry is not changed, the datawill be overwritten in the subsequent run.

4. If spark runs into a out of memory error, note the step the engine has completed and kick off the engine again by commenting the steps already completed under pipeline_config dictionary

5. For Domenican Republic, run NB_Run_Transparency_Engine_DR.ipynb notebook. The activity file just contain item and tenderer information not any buyer or lot, so its config file was slightly modified from the original NB_Run_Transparency_Engine.ipynb script.

6. The final results will be displayed into the synapse lake database and the final pbi report will need to be pointed to this in order to display the final results.

# Steps to Deploy the Beneficial Ownership PowerBI Report
1. Make sure your Beneficial Ownership Consumer is up and running.

2. [Create a Service Principal](https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal)

3. [Enable Service Principals in your Fabric Workspace as a Fabric Admin](https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-service-principal#enable-service-principals)

4. [Enable Access for the Service Prinicpal on your specified workspace](https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-service-principal#workspace-access)


5. Update the [dev variable file](variables/dev.json) with these following variables:
    - `'FabricWorkspaceID'`: The Fabric workspace you want to deploy the PBI Report to.
    - `'BeneficalOwnershipGeneratedDataLake'`: The name of the Synapse Datalake which was generated from running the Benificial Ownership consumer.

6. [Add the following Repository Secrets](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-a-repository) *with the same name*
    - **DEV_FABRIC_SPN_CLIENT_ID** (From Step 1) - [how to find](https://learn.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals#application-object)
    - **DEV_FABRIC_SPN_SECRET** (From Step 1) - [secret value](https://learn.microsoft.com/en-us/azure/active-directory/develop/howto-create-service-principal-portal#option-2-create-a-new-application-secret)
    - **DEV_FABRIC_TENANT_ID** - [how to find](https://learn.microsoft.com/en-us/azure/active-directory/fundamentals/active-directory-how-to-find-tenant#find-tenant-id-through-the-azure-portal)


7. Trigger the **Consumer-PowerBI-Deployment** GitHub Action. If you're unfamiliar with triggering a GitHub Action, follow these [instructions](https://docs.github.com/en/actions/managing-workflow-runs/manually-running-a-workflow).
    - Input **dev** for the "Environment" input
    - Input **beneficial_ownership** for the "Consumer Folder Name" input
