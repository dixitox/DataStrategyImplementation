{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "// Copyright (c) Microsoft Corporation.\n",
        "// Licensed under the MIT license."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import functions as f\n",
        "from delta.tables import *\n",
        "import re\n",
        "import json\n",
        "import datetime\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "raw_folderpath = \"raw/.../\"\n",
        "raw_filename = \"\"\n",
        "primary_key_cols = '[\"column1\",\"column2\", \"etc\"]'\n",
        "partition_cols = '[\"CalcYear\",\"column2\", \"etc\"]'\n",
        "date_partition_column = 'DATECOLUMNNAME'\n",
        "file_type = 'json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# convert parameter partition_cols from string type to list type\n",
        "partition_cols_list = json.loads(partition_cols.replace(\"'\",'\"'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Set to Correct Database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "sparksql"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "execution_finish_time": "2023-05-16T22:09:54.1238893Z",
              "execution_start_time": "2023-05-16T22:09:54.1237096Z",
              "livy_statement_state": "available",
              "parent_msg_id": "e485bfd5-e44a-497b-ad4e-594bf55d7db2",
              "queued_time": "2023-05-16T22:07:06.4083678Z",
              "session_id": "0",
              "session_start_time": null,
              "spark_jobs": null,
              "spark_pool": null,
              "state": "finished",
              "statement_id": -1
            },
            "text/plain": [
              "StatementMeta(, 0, -1, Finished, Available)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [],
              "schema": {
                "fields": [],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 0 rows and 0 fields>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "application/vnd.synapse.sparksql-result+json": {
              "data": [],
              "schema": {
                "fields": [],
                "type": "struct"
              }
            },
            "text/plain": [
              "<Spark SQL result set with 0 rows and 0 fields>"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%sql\n",
        "CREATE DATABASE IF NOT EXISTS staging;\n",
        "USE staging;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Dynamically Get Storage Account Name From Linked Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "storageLinkedService = 'LS_DataLake'\n",
        "storageAccount_ls = mssparkutils.credentials.getPropertiesAll(storageLinkedService)\n",
        "storageAccountName = json.loads(storageAccount_ls)['Endpoint'].split('.')[0].replace('https://','')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Read Input Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "raw_folderpathWithoutContainer = raw_folderpath[4:]\n",
        "if file_type == 'json':\n",
        "    raw_data = spark.read.json(f'abfss://raw@{storageAccountName}.dfs.core.windows.net/{raw_folderpathWithoutContainer}/{raw_filename}')\n",
        "else:\n",
        "    raw_data = spark.read.load(f'abfss://raw@{storageAccountName}.dfs.core.windows.net/{raw_folderpathWithoutContainer}/{raw_filename}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Add Date Partition Columns If Necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if 'CalcYear' in partition_cols_list:\n",
        "    raw_data = raw_data.withColumn('CalcYear', f.year(f.col(date_partition_column)))\n",
        "\n",
        "if 'CalcMonth' in partition_cols_list:\n",
        "    raw_data = raw_data.withColumn('CalcMonth', f.month(f.col(date_partition_column)))\n",
        "\n",
        "if 'CalcDayOfMonth' in partition_cols_list:\n",
        "    raw_data = raw_data.withColumn('CalcDayOfMonth', f.dayofmonth(f.col(date_partition_column)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Set Output Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "stagingFolderPath = re.sub(r'(.*/v[0-9]+/).*', r'\\1', raw_folderpathWithoutContainer)\n",
        "version = re.search(r'\\/(v\\d+)\\/',stagingFolderPath).group(1)\n",
        "stagingFolderPathNoVersion = stagingFolderPath.replace(f'/{version}/','/')\n",
        "stagingAbfssPath = f'abfss://staging@{storageAccountName}.dfs.core.windows.net/{stagingFolderPathNoVersion}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Determine if Delta Table Exists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "tableName = '_'.join(stagingFolderPathNoVersion.split('/')[:-1])\n",
        "\n",
        "tableExists = spark.catalog.tableExists(tableName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Add Filename, Timestamp, Version Columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# version\n",
        "raw_data = raw_data.withColumn('control_file_version',f.lit(version))\n",
        "\n",
        "# location in raw zone\n",
        "location_in_raw = raw_folderpath + raw_filename\n",
        "raw_data = raw_data.withColumn('location_in_raw', f.lit(location_in_raw))\n",
        "\n",
        "# timestamp\n",
        "now = str(datetime.datetime.now())\n",
        "raw_data = raw_data.withColumn('timestamp',f.to_timestamp(f.lit(now)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### If table does **not** exist, create new delta table\n",
        "- Either with or without partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "if tableExists == False:\n",
        "    if partition_cols == \"[]\":\n",
        "        #output with no partition specified\n",
        "        raw_data.write.format(\"delta\") \\\n",
        "                .option(\"path\", stagingAbfssPath) \\\n",
        "                .saveAsTable(tableName)\n",
        "    else:\n",
        "        # output with specified partition\n",
        "        raw_data.write.format(\"delta\") \\\n",
        "                .partitionBy(partition_cols_list) \\\n",
        "                .option(\"path\", stagingAbfssPath) \\\n",
        "                .saveAsTable(tableName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Format primary key columns for merge statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# convert parameter primary_key_cols from string type to list type\n",
        "primary_key_cols_list = json.loads(primary_key_cols.replace(\"'\",'\"'))\n",
        "\n",
        "# set initial mergeOn statement using first primary key columns\n",
        "mergeOn = f'current.{primary_key_cols_list[0]} = new.{primary_key_cols_list[0]}'\n",
        "\n",
        "# Add additional primary key columns to string with preceeding AND\n",
        "for primary_key_col in primary_key_cols_list[1:]:\n",
        "    mergeOn = mergeOn + f' AND current.{primary_key_col} = new.{primary_key_col}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### If Table Exists Merge New Data Into Existing Delta Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.microsoft.delta.schema.autoMerge.enabled\", \"true\")\n",
        "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\",\"true\")\n",
        "\n",
        "if tableExists == True:\n",
        "    deltaTablePointer = DeltaTable.forPath(spark, stagingAbfssPath)\n",
        "\n",
        "    # Drop Null Struct/Array Columns\n",
        "    array_struct_cols = [column[0] for column in raw_data.dtypes if (('struct' in column[1]) | ('array' in column[1]))]\n",
        "    null_cols = [column for column in array_struct_cols if raw_data.select(f.col(column).isNull().cast(\"int\").alias(column)).agg({column: \"sum\"}).collect()[0][0] == 0]\n",
        "    raw_data = raw_data.drop(*null_cols)\n",
        "\n",
        "    # try merge 3 times \n",
        "    try:\n",
        "        deltaTablePointer.alias(\"current\").merge(\n",
        "            raw_data.alias(\"new\"), f\"{mergeOn}\" ) \\\n",
        "            .whenMatchedUpdateAll() \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "            .execute()\n",
        "    except:\n",
        "        # wait 30 seconds\n",
        "        time.sleep(30)\n",
        "        try:\n",
        "            deltaTablePointer.alias(\"current\").merge(\n",
        "            raw_data.alias(\"new\"), f\"{mergeOn}\" ) \\\n",
        "            .whenMatchedUpdateAll() \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "            .execute()\n",
        "        except:\n",
        "            # wait 30 seconds\n",
        "            time.sleep(30)\n",
        "            deltaTablePointer.alias(\"current\").merge(\n",
        "            raw_data.alias(\"new\"), f\"{mergeOn}\" ) \\\n",
        "            .whenMatchedUpdateAll() \\\n",
        "            .whenNotMatchedInsertAll() \\\n",
        "            .execute()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Exit Notebook with Staging Folderpath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "mssparkutils.notebook.exit(stagingFolderPathNoVersion)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
