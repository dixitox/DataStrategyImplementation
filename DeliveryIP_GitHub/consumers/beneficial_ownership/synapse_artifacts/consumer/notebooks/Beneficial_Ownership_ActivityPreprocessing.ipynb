{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "metadata": {},
      "source": [
        "import re\r\n",
        "import math\r\n",
        "from datetime import datetime\r\n",
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.types import StringType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def convert_time_to_period(date: str) -> str:\r\n",
        "    \"\"\"\r\n",
        "    Convert time to a period format like 2018-Q1\r\n",
        "    \"\"\"\r\n",
        "    try:\r\n",
        "        time = datetime.strptime(date, input_time_format)\r\n",
        "        month = time.month\r\n",
        "        year = time.year\r\n",
        "        period = math.ceil(int(month)/period_length)\r\n",
        "        return f'{year}-{period_prefix}{period}'\r\n",
        "    except:\r\n",
        "        return None\r\n",
        "    \r\n",
        "spark.udf.register(\"convert_time\", convert_time_to_period, StringType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "'''\r\n",
        "Convert entity attribute data from wide format to long format.\r\n",
        "Input: \r\n",
        "- Attributes in wide format: [entity, attribute_1, attribute_2, ...]\r\n",
        "- attribute mappings: e.g.\r\n",
        "    \"contact_attribute_mapping\": {\r\n",
        "        \"name\": \r\n",
        "        {\r\n",
        "            \"mapping_cols\": [\"official_name\", \"commercial_name\"],\r\n",
        "            \"output_table\": \"company_names\"\r\n",
        "        },\r\n",
        "        \"address\": \r\n",
        "        {\r\n",
        "            \"mapping_cols\": [\"address\"],\r\n",
        "            \"output_table\": \"company_addresses\"\r\n",
        "        }\r\n",
        "    }\r\n",
        "Output: \r\n",
        "- Attribute data in long format: [entity, attribute, value]\r\n",
        "'''\r\n",
        "\r\n",
        "def convert_to_long_format(\r\n",
        "    attribute_wide_format_data: DataFrame,\r\n",
        "    attribute_mappings: dict,\r\n",
        "    entity_col: str = 'company', \r\n",
        "    time_col: str = None\r\n",
        ") -> DataFrame:\r\n",
        "    # get a list of contact attribute columns\r\n",
        "    attribute_columns = []\r\n",
        "    for attribute in attribute_mappings:\r\n",
        "        attribute_columns.extend(attribute_mappings[attribute]['mapping_cols'])\r\n",
        "\r\n",
        "    # load attribute data\r\n",
        "    if time_col is None:\r\n",
        "        attribute_wide_format_data = attribute_wide_format_data.select([entity_col] + attribute_columns)\r\n",
        "    else:\r\n",
        "        attribute_wide_format_data = attribute_wide_format_data.select([entity_col, time_col] + attribute_columns)\r\n",
        "\r\n",
        "    # convert contact data to long format\r\n",
        "    if time_col is None:\r\n",
        "        melted_attribute_data = melt(\r\n",
        "            data=attribute_wide_format_data,\r\n",
        "            id_vars=[entity_col],\r\n",
        "            value_vars=attribute_columns,\r\n",
        "            var_name='attribute',\r\n",
        "            value_name='value'\r\n",
        "        )\r\n",
        "    else:\r\n",
        "        melted_attribute_data = melt(\r\n",
        "            data=attribute_wide_format_data,\r\n",
        "            id_vars=[entity_col, time_col],\r\n",
        "            value_vars=attribute_columns,\r\n",
        "            var_name='attribute',\r\n",
        "            value_name='value'\r\n",
        "        )\r\n",
        "\r\n",
        "    melted_attribute_data = melted_attribute_data.filter(F.trim('value') != \"\")\r\n",
        "    return melted_attribute_data\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\"\"\"\r\n",
        "Convert dataframe from wide format to long format. Equivalent to pd.melt\r\n",
        "Reference: https://stackoverflow.com/questions/41670103/how-to-melt-spark-dataframe\r\n",
        "\"\"\"\r\n",
        "def melt(\r\n",
        "        data: DataFrame, \r\n",
        "        id_vars: Iterable[str], \r\n",
        "        value_vars: Iterable[str], \r\n",
        "        var_name: str=\"attribute\", \r\n",
        "        value_name: str=\"value\") -> DataFrame:\r\n",
        "    var_value_pairs = F.create_map(\r\n",
        "        list(chain.from_iterable([\r\n",
        "            [F.lit(column), F.col(column)] for column in value_vars]\r\n",
        "        ))\r\n",
        "    )\r\n",
        "    \r\n",
        "    data = data.select(*id_vars, F.explode(var_value_pairs)) \\\r\n",
        "        .withColumnRenamed('key', var_name) \\\r\n",
        "        .withColumnRenamed('value', value_name)\r\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "staging_table = 'INSERT TABLE NAME HERE/DB CONNECTION'\r\n",
        "buyer_join_token = '=='\r\n",
        "## do we need to set a min time?\r\n",
        "input_min_time = ''\r\n",
        "\r\n",
        "### unsure about the partition number\r\n",
        "partitions = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## get from HA these inputs \r\n",
        "\r\n",
        "attribute_dict = properties['attribute_mapping']\r\n",
        "entity_col = property_dict['entity_col']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "property_df = spark.sql(f\"SELECT * FROM {staging_table}\")\r\n",
        "\r\n",
        "activity_data = spark.sql(f\"\"\"\r\n",
        "    SELECT DISTINCT\r\n",
        "        parties_id,\r\n",
        "        awards_id,\r\n",
        "        tender.lots.id,\r\n",
        "        awards_date,\r\n",
        "        convert_time(awards_date) AS {output_time_col},\r\n",
        "        concat(buyer_id, '{buyer_join_token}', buyer_name) AS buyer,\r\n",
        "        item_description\r\n",
        "    FROM {staging_table}\r\n",
        "    WHERE parties_id != '' \r\n",
        "        AND awards_date IS NOT NULL\r\n",
        "        AND TRIM({input_time_col}) >= '{input_min_time}'\r\n",
        "    \"\"\").repartition(partitions)\r\n",
        "    \r\n",
        "activity_data = activity_data.filter(F.col(output_time_col).isNotNull())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "activity_data = activity_data.withColumn('item_description', F.trim(F.regexp_replace(F.col('item_description'), \"[\\\"\\'*()-:;]\", \"\"))) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "melted_activity_data = convert_to_long_format(\r\n",
        "                        attribute_wide_format_data=activity_data,\r\n",
        "                        attribute_mappings=attribute_dict,\r\n",
        "                        entity_col=entity_col,\r\n",
        "                        time_col=output_time_col\r\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "melted_activity_data.write.mode(\"overwrite\").saveAsTable(f'{output_db}.{output_table}')"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}