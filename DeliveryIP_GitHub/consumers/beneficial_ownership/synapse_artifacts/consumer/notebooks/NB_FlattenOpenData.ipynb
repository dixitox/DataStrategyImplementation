{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as f\r\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Set Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "ownership_flag = 1\r\n",
        "contracting_flag = 1\r\n",
        "sanctions_flag = 1\r\n",
        "\r\n",
        "raw_folderpath = \"\"\r\n",
        "country_names = \"\"\r\n",
        "country_name_alternatives = \"\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Create country list\r\n",
        "if country_names:\r\n",
        "    country_names = country_names.split(',')\r\n",
        "\r\n",
        "if country_name_alternatives:\r\n",
        "    country_name_alternatives = country_name_alternatives.split(',')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Test Parameters\r\n",
        "# ownership_flag = 1\r\n",
        "# contracting_flag = 1\r\n",
        "# sanctions_flag = 1\r\n",
        "\r\n",
        "# Contracting\r\n",
        "# raw_folderpath = 'BeneficialOwnership/OpenData/Contracting/*/AllTime/v1/full/*/*/*/*'\r\n",
        "\r\n",
        "# Ownership\r\n",
        "# raw_folderpath = 'BeneficialOwnership/OpenData/Ownership/AllData/v1/full/*/*/*/*'\r\n",
        "\r\n",
        "# Sanctions\r\n",
        "# raw_folderpath = 'BeneficialOwnership/OpenData/Sanctions/AllData/v1/full/*/*/*/*'\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Set Storage Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Storage Config\r\n",
        "storageLinkedService = 'LS_DataLake'\r\n",
        "storageAccount_ls = mssparkutils.credentials.getPropertiesAll(storageLinkedService)\r\n",
        "storageAccountName = json.loads(storageAccount_ls)['Endpoint'].split('.')[0].replace('https://','')\r\n",
        "\r\n",
        "spark.conf.set(f\"spark.storage.synapse.{storageAccountName}.linkedServiceName\",\"LS_DataLake\")\r\n",
        "spark.conf.set(f\"fs.azure.account.oath.provider.type.{storageAccountName}\",\"com.microsoft.azure.synapse.tokenlibrary.LinkedServiceBasedTokenProvider\")\r\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\",True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Define Flatten & Explode Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def flatten_and_explode(df,limit_cols=None):\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    Flattens any struct or array columns in a PySpark DataFrame recursively.\r\n",
        "    Inputs: df = dataframe to be flattened, limit_cols = specific columns to be processed (optional, speeds up the process)\r\n",
        "    \"\"\"\r\n",
        "    array_cols = [c[0] for c in df.dtypes if c[1][:5] == 'array']\r\n",
        "    struct_cols = [c[0] for c in df.dtypes if c[1][:6] == 'struct']\r\n",
        "    other_cols = [c[0] for c in df.dtypes if ((c[1][:5] != 'array') & (c[1][:6] != 'struct'))]\r\n",
        "\r\n",
        "    if limit_cols == None:\r\n",
        "        limit_cols_updt = [c[0] for c in df.dtypes]\r\n",
        "    else:\r\n",
        "        limit_cols_updt = limit_cols\r\n",
        "\r\n",
        "    while 1==1:\r\n",
        "\r\n",
        "        if len(array_cols) == 0 and len(struct_cols) == 0:\r\n",
        "            break\r\n",
        "\r\n",
        "        # Flatten struct columns\r\n",
        "        for c in struct_cols:\r\n",
        "            df = df.select(other_cols+array_cols + struct_cols + [f.col(c + '.' + x).alias(c + '_' + x) for x in df.select(c + '.*').columns])\r\n",
        "            df = df.drop(c)\r\n",
        "\r\n",
        "            struct_cols = [c[0] for c in df.dtypes if ((c[1][:6] == 'struct') & (c[0] in limit_cols_updt))]\r\n",
        "            array_cols = [c[0] for c in df.dtypes if ((c[1][:5] == 'array') & (c[0] in limit_cols_updt))]\r\n",
        "            other_cols = [c[0] for c in df.dtypes if ((c[1][:5] != 'array') & (c[1][:6] != 'struct') & (c[0] in limit_cols_updt))]\r\n",
        "\r\n",
        "            print('processed struct col '+str(c))\r\n",
        "\r\n",
        "        # Explode array columns\r\n",
        "        for c in array_cols:\r\n",
        "            df_tonotexplode = df.where((f.col(c).isNull()) | (f.size(f.col(c)) ==0)).drop(c)\r\n",
        "            df_toexplode = df.where((f.col(c).isNotNull()) & (f.size(f.col(c)) > 0)).withColumn(c, f.explode(f.col(c)))\r\n",
        "            df = df_toexplode.unionByName(df_tonotexplode, allowMissingColumns=True)\r\n",
        "\r\n",
        "            print('processed array col '+str(c))\r\n",
        "\r\n",
        "        # Update columns to be processed\r\n",
        "        if limit_cols == None:\r\n",
        "            limit_cols_updt = [c[0] for c in df.dtypes]\r\n",
        "        else:\r\n",
        "            limit_cols_updt = limit_cols\r\n",
        "\r\n",
        "        # Update column lists for next loop\r\n",
        "        array_cols = [c[0] for c in df.dtypes if ((c[1][:5] == 'array') & (c[0] in limit_cols_updt))]\r\n",
        "        struct_cols = [c[0] for c in df.dtypes if ((c[1][:6] == 'struct') & (c[0] in limit_cols_updt))]\r\n",
        "        other_cols = [c[0] for c in df.dtypes if ((c[1][:5] != 'array') & (c[1][:6] != 'struct') & (c[0] in limit_cols_updt))]\r\n",
        "\r\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def df_select_columns(source_df, selected_columns):\r\n",
        "    existing_columns = source_df.columns\r\n",
        "    columns_to_select = []\r\n",
        "    \r\n",
        "    for column in selected_columns:\r\n",
        "        if column in existing_columns:\r\n",
        "            columns_to_select.append(column)\r\n",
        "        else:\r\n",
        "            columns_to_select.append(f.lit(\"\").alias(column))\r\n",
        "    \r\n",
        "    df = source_df.select(columns_to_select)\r\n",
        "    renamed_columns = [f.col(column_name).alias(column_name.replace(\"_\", \"\")) for column_name in df.columns]\r\n",
        "    selected_df = df.select(*renamed_columns)\r\n",
        "\r\n",
        "    return selected_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Read, Flatten, Write Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#OpenContracting\r\n",
        "if contracting_flag==1:\r\n",
        "    #Read\r\n",
        "    if country_names:\r\n",
        "        try:\r\n",
        "            df = spark.read.json('abfss://raw@actstaxdatalakedev60.dfs.core.windows.net/'+raw_folderpath+'*').withColumn('filename',f.input_file_name()).where(f.col('filename').rlike(\"|\".join(country_names)))\r\n",
        "        except:\r\n",
        "            df = spark.read.json('abfss://raw@actstaxdatalakedev60.dfs.core.windows.net/'+raw_folderpath+'*').withColumn('filename',f.input_file_name()).where(f.col('filename').rlike(\"|\".join(country_names)))\r\n",
        "    else:\r\n",
        "        try:\r\n",
        "            df = spark.read.json('abfss://raw@actstaxdatalakedev60.dfs.core.windows.net/'+raw_folderpath+'*').withColumn('filename',f.input_file_name())\r\n",
        "        except:\r\n",
        "            df = spark.read.json('abfss://raw@actstaxdatalakedev60.dfs.core.windows.net/'+raw_folderpath+'*').withColumn('filename',f.input_file_name())\r\n",
        "            \r\n",
        "    #Select, Flatten, and Explode\r\n",
        "    df = df_select_columns(df,['date','parties','tender','awards','filename'])\r\n",
        "    limit_cols = ['date','parties','parties_address','parties_address_streetAddress','parties_address_postalCode','parties_address_countryName','parties_id','parties_name','parties_contactPoint','parties_contactPoint_email','parties_contactPoint_telephone','parties_contactPoint_url','parties_roles','tender','tender_id','tender_title','awards','awards_items','awards_items_id','awards_items_description','tender_lots','tender_lots_id','tender_lots_description','filename']\r\n",
        "    flat_df = flatten_and_explode(df,limit_cols)\r\n",
        "\r\n",
        "    #Write NonNull Records\r\n",
        "    select_cols = ['date','parties_address_streetAddress','parties_address_postalCode','parties_address_countryName','parties_id','parties_name','parties_contactPoint_email','parties_contactPoint_telephone','parties_contactPoint_url','parties_roles','tender_id','tender_title','awards_items_id','awards_items_description','tender_lots_id','tender_lots_description','filename']\r\n",
        "    df_select = df_select_columns(flat_df,select_cols)\r\n",
        "\r\n",
        "    df_select_notnull = df_select.where(f.col('partiesid').isNotNull() & f.col('date').isNotNull() & f.col('partiesroles').isNotNull() & f.col('tenderid').isNotNull())\r\n",
        "    df_select_notnull.write.mode('overwrite').parquet('abfss://staging@actstaxdatalakedev60.dfs.core.windows.net/BeneficialOwnership/OpenData/Contracting/Flattened')\r\n",
        "\r\n",
        "    #Write Null Records\r\n",
        "    df_select_null = df_select.where(f.col('partiesid').isNull() | f.col('date').isNull() | f.col('partiesroles').isNull() | f.col('tenderid').isNull())\r\n",
        "    df_select_null.write.mode('overwrite').parquet('abfss://staging@actstaxdatalakedev60.dfs.core.windows.net/BeneficialOwnership/OpenData/Contracting/ContractingDF_FailedPreProcessing_Staging')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "#OpenOwnership\r\n",
        "if ownership_flag==1:\r\n",
        "    #Read\r\n",
        "    try:\r\n",
        "        df = spark.read.json('abfss://raw@actstaxdatalakedev60.dfs.core.windows.net/'+raw_folderpath+'*')\r\n",
        "    except:\r\n",
        "        df = spark.read.json('abfss://raw@actstaxdatalakedev60.dfs.core.windows.net/'+raw_folderpath+'*')\r\n",
        "    \r\n",
        "    #Select, Flatten, and Explode\r\n",
        "\r\n",
        "    #Entity\r\n",
        "    df_entity = df_select_columns(df.where(f.col('statementType')=='entityStatement'),['statementID','statementType','name','addresses','identifiers'])\r\n",
        "    limit_cols = ['statementID','statementType','name','identifiers','addresses','addresses_address','addresses_country','addresses_type','identifiers_id']\r\n",
        "    entity_flat = flatten_and_explode(df_entity,limit_cols)\r\n",
        "\r\n",
        "    #Limit country\r\n",
        "    if country_names:\r\n",
        "        country_name_alternatives.append(\",\".join(country_names))\r\n",
        "        entity_flat = entity_flat.where(f.col('addresses_country').isin(country_name_alternatives))\r\n",
        "\r\n",
        "    #Write NonNull Records\r\n",
        "    select_cols = ['statementID','statementType','name','addresses_address','addresses_country','addresses_type','identifiers_id']\r\n",
        "    df_select_entity = df_select_columns(entity_flat,select_cols)\r\n",
        "    df_select_entity_notnull = df_select_entity.where(f.col('identifiersid').isNotNull() & f.col('addressesaddress').isNotNull())\r\n",
        "    df_select_entity_notnull.write.mode('overwrite').parquet('abfss://staging@actstaxdatalakedev60.dfs.core.windows.net/BeneficialOwnership/OpenData/Ownership/Entity/Flattened')\r\n",
        "\r\n",
        "    #Write Null Records\r\n",
        "    df_select_entity_null = df_select_entity.where(f.col('identifiersid').isNull() | f.col('addressesaddress').isNull())\r\n",
        "    df_select_entity_null.write.mode('overwrite').parquet('abfss://staging@actstaxdatalakedev60.dfs.core.windows.net/BeneficialOwnership/OpenData/Ownership/Entity/OwnershipDF_FailedPreProcessing')\r\n",
        "\r\n",
        "    #Person\r\n",
        "    df_person = df_select_columns(df.where(f.col('statementType')==f.lit('personStatement')),['statementID','statementType','identifiers','personType','addresses','names','nationalities'])\r\n",
        "    limit_cols = ['statementID','statementType','names','identifiers','identifiers_id','addresses','addresses_address','nationalities','personType','names_fullName','nationalities_name','nationalities_code']\r\n",
        "    person_flat = flatten_and_explode(df_person,limit_cols)\r\n",
        "\r\n",
        "    #Limit country\r\n",
        "    if country_names:\r\n",
        "        country_name_alternatives.append(\",\".join(country_names))\r\n",
        "        person_flat = person_flat.where(f.col('addresses_address').rlike(\"|\".join(country_name_alternatives)))\r\n",
        "    \r\n",
        "    #Write NonNull Records\r\n",
        "    select_cols = ['statementID','statementType','identifiers_id','personType','addresses_address','names_fullName','nationalities_name','nationalities_code']\r\n",
        "    df_select_person = df_select_columns(person_flat,select_cols)\r\n",
        "    df_select_person_notnull = df_select_person.where(f.col('identifiersid').isNotNull() & f.col('namesfullName').isNotNull())\r\n",
        "    df_select_person_notnull.write.mode('overwrite').parquet('abfss://staging@actstaxdatalakedev60.dfs.core.windows.net/BeneficialOwnership/OpenData/Ownership/Person/Flattened')\r\n",
        "    \r\n",
        "    #Write Null Records\r\n",
        "    df_select_person_null = df_select_person.where(f.col('identifiersid').isNull() | f.col('namesfullName').isNull())\r\n",
        "    df_select_person_null.write.mode('overwrite').parquet('abfss://staging@actstaxdatalakedev60.dfs.core.windows.net/BeneficialOwnership/OpenData/Ownership/Person/OwnershipDF_FailedPreProcessing')\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#OpenSanctions\r\n",
        "if sanctions_flag==1:\r\n",
        "    #Read\r\n",
        "    df = spark.read.json('abfss://raw@actstaxdatalakedev60.dfs.core.windows.net/'+raw_folderpath+'*')\r\n",
        "\r\n",
        "    # Flatten according to schema (faster than all at once)\r\n",
        "\r\n",
        "    # Company and Person\r\n",
        "    df_company_person = df_select_columns(df.where((f.col('schema') == 'Company') | (f.col('schema') == 'Person')),['caption','id','referents','schema','properties'])\r\n",
        "    limit_cols = ['caption','id','referents','schema','properties','properties_name','properties_addressEntity']\r\n",
        "    company_person_flat = flatten_and_explode(df_company_person,limit_cols).select('caption','id','referents','schema','properties_name','properties_addressEntity')\r\n",
        "\r\n",
        "    # Address\r\n",
        "    df_address = df_select_columns(df.where(f.col('schema')=='Address'),['caption','id','referents','schema','properties'])\r\n",
        "    limit_cols = ['caption','id','referents','schema','properties','properties_full']\r\n",
        "    address_flat = flatten_and_explode(df_address,limit_cols).select('caption','id','referents','schema','properties_full')\r\n",
        "\r\n",
        "    if country_names:\r\n",
        "        country_name_alternatives.append(\",\".join(country_names))\r\n",
        "        address_flat = address_flat.where(f.col('properties_full').rlike(\"|\".join(country_name_alternatives)))\r\n",
        "\r\n",
        "    # Sanction\r\n",
        "    df_sanction = df_select_columns(df.where(f.col('schema')=='Sanction'),['caption','id','referents','schema','properties'])\r\n",
        "    limit_cols = ['caption','id','referents','schema','properties','properties_entity','properties_startDate','properties_endDate','properties_reason','properties_authority']\r\n",
        "    sanction_flat = flatten_and_explode(df_sanction,limit_cols).select('caption','id','referents','schema','properties_entity','properties_startDate','properties_endDate','properties_reason','properties_authority')\r\n",
        "\r\n",
        "\r\n",
        "    # Join Sanction to Person/Company to get addressEntity\r\n",
        "    # join on id\r\n",
        "    sanction_id = sanction_flat.join(company_person_flat,sanction_flat.properties_entity==company_person_flat.id, how='inner').select(sanction_flat.caption,sanction_flat.id,'properties_name','properties_entity','properties_startDate','properties_endDate','properties_reason','properties_authority','properties_addressEntity')\r\n",
        "\r\n",
        "    # join on referents\r\n",
        "    sanction_referents = sanction_flat.join(company_person_flat,sanction_flat.properties_entity==company_person_flat.referents, how='inner').select(sanction_flat.caption,sanction_flat.id,'properties_name','properties_entity','properties_startDate','properties_endDate','properties_reason','properties_authority','properties_addressEntity')\r\n",
        "\r\n",
        "    # append\r\n",
        "    sanction_append = sanction_id.unionByName(sanction_referents, allowMissingColumns=True).distinct()\r\n",
        "\r\n",
        "    # Join to Address to get full Address\r\n",
        "    # join on addressEntity\r\n",
        "    sanction_join = sanction_append.join(address_flat,sanction_append.properties_addressEntity==address_flat.id,how='left').select(sanction_append.id,sanction_append.caption,'properties_name','properties_entity','properties_startDate','properties_endDate','properties_reason','properties_authority','properties_full').distinct()\r\n",
        "\r\n",
        "\r\n",
        "    #Write NonNull Records\r\n",
        "    renamed_columns = [f.col(column_name).alias(column_name.replace(\"_\", \"\")) for column_name in sanction_join.columns]\r\n",
        "    df_select_sanctions_notnull = sanction_join.where(f.col('id').isNotNull() & f.col('caption').isNotNull()).select(*renamed_columns)\r\n",
        "    df_select_sanctions_notnull.write.mode('overwrite').parquet('abfss://staging@actstaxdatalakedev60.dfs.core.windows.net/BeneficialOwnership/OpenData/Sanctions/Flattened')\r\n",
        "    \r\n",
        "    #Write Null Records\r\n",
        "    df_select_sanctions_null = sanction_join.where(f.col('id').isNull() | f.col('caption').isNull()).select(*renamed_columns)\r\n",
        "    df_select_sanctions_null.write.mode('overwrite').parquet('abfss://staging@actstaxdatalakedev60.dfs.core.windows.net/BeneficialOwnership/OpenData/Sanctions/SanctionsDF_FailedPreProcessing')\r\n",
        ""
      ]
    }
  ],
  "metadata": {
    "description": null,
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}